# CD to AWS EKS using GHCR images
# ------------------------------------------------------------
# REQUIREMENTS (set these in GitHub > Settings > Secrets and variables > Actions):
#   AWS_ACCESS_KEY_ID        -> IAM user/role with EKS + ECR read permissions (EKS:DescribeCluster is required)
#   AWS_SECRET_ACCESS_KEY    -> matching secret key (skip if you use OIDC)
#   AWS_REGION               -> e.g., eu-central-1
#   EKS_CLUSTER_NAME         -> e.g., fm-eks
#
# NOTES:
# - This workflow builds and pushes your app image to GHCR (ghcr.io).
# - It then connects to your EKS cluster and deploys the manifests from k8s/ folder.
# - If your GHCR package is PRIVATE, add an imagePullSecret (see Deployment comments).
# - Deployment name and container name must match the values below.
# ------------------------------------------------------------

name: CD to EKS via GHCR

on:
  push:
    branches: [ "main" ]     # deploy on every push to main
  workflow_dispatch:          # allow manual runs

jobs:
  deploy:
    runs-on: ubuntu-latest

    permissions:
      id-token: write         # future-proof (for AWS OIDC if you switch later)
      contents: read
      packages: write         # needed to push to GHCR

    env:
      AWS_REGION: ${{ secrets.AWS_REGION }}
      EKS_CLUSTER_NAME: ${{ secrets.EKS_CLUSTER_NAME }}
      K8S_NAMESPACE: demo
      DEPLOYMENT_NAME: financial-app
      CONTAINER_NAME: app
      IMAGE_URI: ghcr.io/katzchaim/financial-management
      IMAGE_TAG_SHA: ${{ github.sha }}

    steps:
      # 1) Checkout code
      - name: Checkout
        uses: actions/checkout@v4

      # 2) Login to GHCR so we can push images (uses repository GITHUB_TOKEN)
      - name: Log in to GHCR
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      # 3) Build & push image to GHCR (latest + commit SHA)
      - name: Build & push image to GHCR
        uses: docker/build-push-action@v6
        with:
          push: true
          tags: |
            ${{ env.IMAGE_URI }}:${{ env.IMAGE_TAG_SHA }}
            ${{ env.IMAGE_URI }}:latest

      # 4) Configure AWS credentials (Access Keys method).
      #    If you move to OIDC, replace this with role-to-assume.
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      # 5) Generate kubeconfig for EKS on-the-fly
      - name: Update kubeconfig for EKS
        run: |
          aws eks update-kubeconfig --name "${EKS_CLUSTER_NAME}" --region "${AWS_REGION}"
          kubectl get nodes

      # 6) Ensure namespace exists
      - name: Ensure namespace exists
        run: |
          kubectl create ns "${K8S_NAMESPACE}" --dry-run=client -o yaml | kubectl apply -f -

      # 7) Apply Kubernetes manifests from k8s/ directory
      - name: Apply manifests
        run: |
          kubectl apply -n "${K8S_NAMESPACE}" -f k8s/ --validate=false

      # 8) Force the Deployment to use the freshly built SHA image (avoid "latest" drift)
      - name: Pin image to SHA and rollout
        run: |
          kubectl set image -n "${K8S_NAMESPACE}" deployment/${DEPLOYMENT_NAME} ${CONTAINER_NAME}=${IMAGE_URI}:${IMAGE_TAG_SHA}
          kubectl rollout status -n "${K8S_NAMESPACE}" deployment/${DEPLOYMENT_NAME} --timeout=300s
          kubectl get deploy,po -n "${K8S_NAMESPACE}" -o wide
